![](../Images/Pasted%20image%2020230929182930.png)
- This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation.
- As single LLM calls often lack the ability to reflect, possess no working memory or scratch pad, and cannot act or perceive beyond the knowledge obtained from their training data, augmenting language models with tools that facilitate perception and action, such as the plugin that enables ChatGPT+ to utilize fresh web pages from the internet, can significantly improve performance.
- Despite the remarkable success the LLM-with-tools paradigm has enabled, it typically utilizes a single LLM agent. Considering the evolving range of real-world tasks that could benefit from LLMs and the intrinsic weaknesses of using a single agent, a promising direction for future LLM applications is to have multiple agents work together to solve complex tasks.
- More agents could be introduced to -
	- incorporate different roles that encourage divergent thinking, improve factuality and reasoning with LLMs, through multi-agent debate.
	- allow effective tool usage and execution with potentially autonomous troubleshooting through inter-agent interactions
	- realize human agency, etc. In such multi-agent systems, inter-agent conversations will be crucial, using language (natural or code) as means to enable agent collaboration through potential multi-round, back-and-forth message exchanges. These conversations could leverage the recent chat-based LLMs’ capability to communicate and incorporate feedback from others at human level
- AutoGen has the following key features:
	- <span style="color:#ffc000">Customizable agents that integrate LLMs, humans, and tools</span> - By selecting and configuring a subset of built-in capabilities, one can easily create agents with different roles
	- <span style="color:#ffc000">Conversable agents with unified conversation interface</span> - To realize this, every Autogen agent is made conversable – they can receive, react, and respond to messages. This design (1) leverages the strong capability of the most advanced LLMs in taking feedback and making progress via chat and (2) allows humans to participate flexibly during an active inter-agent conversation, enabling both human agency and automation
![](../Images/Pasted%20image%2020230929185553.png)
## The AutoGen Framework

- AutoGen abstracts and implements conversable agents designed to solve tasks through inter-agent conversations. Using conversable agents in AutoGen, developers can create various forms and patterns of multi-agent conversations involving LLMs, humans, and tools.
	- defining a set of conversable agents with specialized capabilities and roles
	- defining the interaction behavior between agents, i.e., how an agent should respond when receiving messages from another agent
### Conversable Agent
 - A conversable agent is an entity with a specific role that can send and receive messages to and from other conversable agents to start or continue a conversation. It maintains its internal states based on sent and received messages and can be configured with a set of capabilities The agent can act according to programmed behavior patterns.
 - An agent’s capability directly influences how the agent processes and responds to messages. An agent powered by a highly capable LLM has the abilities of language understanding, generation, and some reasoning. Besides LLM, agents in AutoGen can also be powered by tools or humans to overcome LLM limitations and harness human intelligence.
 - The main goal of Autogen is to enable next-gen LLM applications through multi-agent conversations, making LLM a critical component as the back-end of agents in Autogen. Different agents can be backed by various LLM configurations. For instance, some may use LLMs or configurations tuned on private data. Additionally, LLMs can be set up to play different roles with distinct system messages![](../Images/Pasted%20image%2020230930230150.png)
 - AutoGen supports the use of widely used LLM inference APIs and offers enhanced and optimized LLM inference settings. It also includes various improved features, such as result caching, error handling, message templating, etc. via an enhanced LLM inference layer.
 - To effectively address user needs, next-gen LLM applications should enable easy integration of human feedback and involvement at different levels. This motivates Autogen to include human as an agent back-end. AutoGen lets a human participate in agent conversation via a proxy agent, which has the same conversation interface as any other agent
 - AutoGen also allows configurable human involvement levels and patterns, including the frequency and conditions for requesting human input, and the option for humans to skip providing input. This enables varying degrees of autonomy
 - Using tools is an effective way to overcome LLM limitations. AutoGen natively supports a generic form of tool usage through code generation and execution. Specifically, when using a default assistant agent from AutoGen, the system message prompts the LLM to suggest Python code or shell scripts to solve problems in several cases. These cases cover common scenarios where tools are needed, such as situations requiring information collection. The system message is carefully designed to enable intelligent multi-step problem-solving and utilize the conversational process to recover from failure. Additionally, agents in AutoGen support making LLM-suggested function calls, leveraging the new feature of OpenAI models to use pre-defined toolsets.
 - A unique feature in AutoGen is its ability to separate code generation and execution across different agents, i.e., using one agent to generate code or function calls and another agent to execute the code or function calls and respond. This separation simplifies simulation of real-world conversational processes involving multiple steps, such as human or AI review, API key replacement, or debugging, through potentially multi-turn conversations.
 - When a human wants to skip providing feedback, a Python code executor is called upon to reply the execution results or error messages
 - With built-in support for agent capabilities, AutoGen allows one to easily create agents with specialized capabilities and roles. When building a multi-agent system, one can either directly reuse the built-in agents in AutoGen or develop customized agents based on them.
### Multi-Agent Conversations
- <span style="color:#ffc000">unified conversation interfaces</span> - Agents in AutoGen have unified conversation interfaces, including send/receive for sending/receiving messages and generate reply for generating a reply based on the received message. With this conversation-centric design, a workflow can be represented as a sequence of inter-agent message passing and agent acting. The LLM-backed agents can use the chat history to perform LLM inference, which automatically infers the current state and respond. That can greatly reduce the need for complex state management which can be a major burden of development. We hypothesize that this is a key approach to simplify a traditional complex workflow in general: a developer can design a system in which LLM-backed agents drive complex workflows based on natural language instructions and chat history, while tool-backed agents handle structured instructions supported by programming languages.
- <span style="color:#ffc000">Automated multi-agent conversation</span> - To ease the development of multi-agent conversation, we aim to reduce the developers’ effort to only defining the behavior of each agent. That is, once agents are appropriately configured, the developer can readily trigger the conversation among the agents and the conversation would proceed automatically with no extra effort of the developer for crafting a control plane. Towards that goal, we introduce and, by default, adopt an agent auto-reply mechanism to enable automated multi-agent conversation: Once an agent receives a message from another agent, it automatically invokes generate reply and sends the reply back to the sender unless it is empty. This simple mechanism enables automated agent chat, i.e., the agent conversation can proceed automatically after the conversation is initialized, without any extra control plane needed. ![](../Images/Pasted%20image%2020230930232810.png)
- If a developer needs to introduce an additional agent to an existing agent workflow, the developer only needs to program the added agent and modifies the agents that may have conversations with it
- <span style="color:#ffc000">Fully autonomous conversations</span> - One can achieve fully autonomous conversations with a minimum initialization step in AutoGen. In particular, one only needs to set the ‘human input mode’ to ‘NEVER’ in agents and register auto-reply functions properly
- <span style="color:#ffc000">Static and dynamic conversations</span> - The dynamic conversation pattern is useful in complex applications where the patterns of interaction cannot be predetermined in advance. The support of dynamic conversation patterns is rare in existing multi-agent LLM systems.
	- Registered auto-reply: With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. In the system, we register an auto-reply function in the group chat manager, which lets LLM decide who will the next speaker be in a group chat setting. Depending on the topic of the group chat discussion, one speaker may or may not be selected to speak.
	- Function call: In this approach, LLM decides whether or not to call a particular function depending on the conversation status in each inference call. By messaging additional agents in the called functions, the LLM can drive dynamic multi-agent conversation.
## Related Work
- <span style="color:#ffc000">Single Agent Systems</span>
	- <span style="color:#ffc000">Auto-GPT</span>: Auto-GPT is an open-source implementation of an AI agent that could autonomously achieve a given goal . It follows a single-agent paradigm in which it augments the AI model with many useful tools and does not support multi-agent collaboration.
	- <span style="color:#ffc000">ChatGPT+ (with code interpreter or plugin)</span>
	- <span style="color:#ffc000">LangChain Agents</span> - LangChain is a general framework for developing LLM-based applications. LangChain Agents is a subpackage for using an LLM to choose a sequence of actions
- <span style="color:#ffc000">Multiple Agent Systems</span> - 
	- <span style="color:#ffc000">BabyAGI</span> - BabyAGI is an example implementation of an AI-powered task management system in a Python script (according to its own documentation). In this implemented system, multiple LLM-based agents are used. For example, there is an agent for creating new tasks based on the objective and the result of the previous task, an agent for prioritizing the task list, and an agent for completing tasks/sub-tasks. BabyAGI is a multi-agent system with a static agent communication pattern, i.e., a predefined order of agent communication
	- <span style="color:#ffc000">CAMEL</span> - It demonstrates how role-playing can be used to let chat agents communicate with each other for task completion. It also records agent conversations for behavior analysis and capability understanding. CAMEL does not support tool-using, such as code execution. Inception prompting technique is used to achieve autonomous cooperation between agents
	- <span style="color:#ffc000">MetaGPT</span> - It is a specialized LLM-based multi-agent framework for collaborative software development
	- <span style="color:#ffc000">Multi-Agent Debate</span> - Two recent works investigate and show that multi-agent debate is an effective way to encourage divergent thinking in LLMs and to improve the factuality and reasoning of LLMs. In both works, multiple LLM inference instances are constructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM inference instance, while no tool or human is involved. The conversation also needs to follow a pre-defined order.
![](../Images/Pasted%20image%2020231001161005.png)
## Future Work

 - <span style="color:#ffc000">Designing Optimal Multi-Agent Workflows</span> - Creating a multi-agent workflow for a given task can involve many decisions, e.g., how many agents to include, how to assign agent roles and agent capabilities, how should the agents interact with each other, and whether to automate a particular part of the workflow. There may not exist a one-fit-all answer and the best solution might depend on the specific application
	 - For what types of tasks and applications are multi-agent workflows most useful?
	 - How do multi-agents help in different applications?
	 - For a given task, what is the optimal (e.g., cost-effective) multi-agent workflow?
- <span style="color:#ffc000">Creating Highly Capable Agents</span> - AutoGen can enable development of highly capable agents that can leverage strengths of LLMs, tools, and humans. Creating such agents is crucial to ensure that a multi-agent workflow can effectively troubleshoot and make progress on a task.