- what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios.
- Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress.![](../Images/Pasted%20image%2020230928192619.png)
- a general conceptual framework for the LLMbased agents with three key parts: 
	- <span style="color:#ffc000">brain</span> - the brain is the core of an AI agent because it not only stores crucial memories, information, and knowledge but also undertakes essential tasks of information processing, decision-making, reasoning, and planning.
	- <span style="color:#ffc000">perception</span> - this module serves a role similar to that of sensory organs for humans. Its primary function is to expand the agent’s perceptual space from text-only to a multimodal space that includes diverse sensory modalities like text, sound, visuals, touch, smell, and more. This expansion enables the agent to better perceive information from the external environment.
	- <span style="color:#ffc000">action</span> - we expect the agent to be able to possess textual output, take embodied actions, and use tools so that it can better respond to environmental changes and provide feedback, and even alter and shape the environment
- Due to the seemingly metaphysical nature of concepts like consciousness and desires for computational entities, and given that we can only observe the behavior of the machine, many AI researchers, including Alan Turing, suggest temporarily setting aside the question of whether an agent is “actually” thinking or literally possesses a “mind”. Instead, researchers employ other attributes to help describe an agent, such as properties of autonomy, reactivity, pro-activeness and social ability. There are also researchers who held that intelligence is “in the eye of the beholder”; it is not an innate, isolated property
## Technological Trends in Agent Research

- <span style="color:#ffc000">Symbolic Agents</span> - They possess explicit and interpretable reasoning frameworks, and due to their symbolic nature, they exhibit a high degree of expressive capability. A classic example of this approach is knowledge-based expert systems. However, symbolic agents faced limitations in handling uncertainty and large-scale real-world problems
- <span style="color:#ffc000">Reactive agents</span> - These agents are mainly based on a sense-act loop, efficiently perceiving and reacting to the environment. The design of such agents prioritizes direct input-output mappings rather than intricate reasoning and symbolic operations. They typically require fewer computational resources, enabling quicker responses, but they might lack complex higher-level decision-making and planning capabilities
- <span style="color:#ffc000">Reinforcement learning-based agents</span> - The primary concern in this field is how to enable agents to learn through interactions with their environments, enabling them to achieve maximum cumulative rewards in specific tasks. Initially, reinforcement learning (RL) agents were primarily based on fundamental techniques such as policy search and value function optimization, exemplified by Q-learning. With the rise of deep learning, the integration of deep neural networks and reinforcement learning, known as Deep Reinforcement Learning (DRL), has emerged. Nonetheless, reinforcement learning faces challenges including long training times, low sample efficiency, and stability concerns, particularly when applied in complex real-world environments
- <span style="color:#ffc000">Agents with transfer learning and meta learning</span> - Traditionally, training a reinforcement learning agent requires huge sample sizes and long training time, and lacks generalization capability. Consequently, researchers have introduced transfer learning to expedite an agent’s learning on new tasks. Transfer learning reduces the burden of training on new tasks and facilitates the sharing and migration of knowledge across different tasks, thereby enhancing learning efficiency, performance, and generalization capabilities
  
  Meta-learning focuses on learning how to learn, enabling an agent to swiftly infer optimal policies for new tasks from a small number of samples. Such an agent, when confronted with a new task, can rapidly adjust its learning approach by leveraging acquired general knowledge and policies, consequently reducing the reliance on a large volume of samples. However, when there exist significant disparities between source and target tasks, the effectiveness of transfer learning might fall short of expectations and there may exist negative transfer. . Additionally, the substantial amount of pre-training and large sample sizes required by meta learning make it hard to establish a universal learning policy
- <span style="color:#ffc000">Large language model-based agents</span> - large language models undergo pre-training on large-scale corpora and demonstrate the capacity for few-shot and zero-shot generalization, allowing for seamless transfer between tasks without the need to update parameters. Furthermore, research suggests that allowing multiple agents to coexist can lead to the emergence of social phenomena
	- <span style="color:#ffc000">Autonomy</span> - LLMs can demonstrate a form of autonomy through their ability to generate human-like text, engage in conversations, and perform various tasks without detailed step-by-step instructions. Moreover, they can dynamically adjust their outputs based on environmental input, reflecting a degree of adaptive autonomy. They can showcase autonomy through exhibiting creativity like coming up with novel ideas, stories, or solutions that haven’t been explicitly programmed into them. This implies a certain level of self-directed exploration and decision-making. Simply by providing them with a task and a set of available tools, they can autonomously formulate plans and execute them to achieve the ultimate goal.
	- <span style="color:#ffc000">Reactivity</span> - Traditionally, the perceptual space of language models has been confined to textual inputs, while the action space has been limited to textual outputs. However, researchers have demonstrated the potential to expand the perceptual space of LLMs using multimodal fusion techniques, enabling them to rapidly process visual and auditory information from the environment. One major challenge is that LLM-based agents, when performing non-textual actions, require an intermediate step of generating thoughts or formulating tool usage in textual form before eventually translating them into concrete actions. This intermediary process consumes time and reduces the response speed. However, this aligns closely with human behavioral patterns, where the principle of “think before you act” is observed
	- <span style="color:#ffc000">Pro-activeness</span> - This property emphasizes that agents can reason, make plans, and take proactive measures in their actions to achieve specific goals or adapt to environmental changes. Although intuitively the paradigm of next token prediction in LLMs may not possess intention or desire, research has shown that they can implicitly generate representations of these states and guide the model’s inference process. LLMs have demonstrated a strong capacity for generalized reasoning and planning. By prompting large language models with instructions like “let’s think step by step”, we can elicit their reasoning abilities, such as logical and mathematical reasoning
	- <span style="color:#ffc000">Social ability</span> - Large language models exhibit strong natural language interaction abilities like understanding and generation. Many researchers have demonstrated that LLM-based 9 agents can enhance task performance through social behaviors such as collaboration and competition. By inputting specific prompts, LLMs can also play different roles, thereby simulating the social division of labor in the real world. Furthermore, when we place multiple agents with distinct identities into a society, emergent social phenomena can be observed
![](../Images/Pasted%20image%2020230928201610.png)
## Brain

To ensure effective communication, the ability to engage in natural language interaction is paramount. After receiving the information processed by the perception module, the brain module first turns to storage, retrieving in knowledge and recalling from memory. These outcomes aid the agent in devising plans, reasoning, and making informed decisions. Additionally, the brain module may memorize the agent’s past observations, thoughts, and actions in the form of summaries, vectors, or other data structures. Meanwhile, it can also update the knowledge such as common sense and domain knowledge for future use. The LLM-based agent may also adapt to unfamiliar scenarios with its inherent generalization and transfer ability
![](../Images/Pasted%20image%2020230928202013.png)
### Natural Language Interaction

- The capability of multi-turn conversation is the foundation of effective and consistent communication
- However, even humans find it hard to communicate without confusion in one sitting, so multiple rounds of dialogue are necessary. Compared with traditional text-only reading comprehension tasks like SQuAD, <span style="color:#ffc000">multi-turn conversations</span> (1) are interactive, involving multiple speakers, and lack continuity; (2) may involve multiple topics, and the information of the dialogue may also be redundant, making the text structure more complex
- In general, the multi-turn conversation is mainly divided into three steps: (1) Understanding the history of natural language dialogue, (2) Deciding what action to take, and (3) Generating natural language responses.
- Importantly, <span style="color:#ffc000">they do not merely copy training data</span> but display a certain degree of creativity, generating diverse texts that are equally novel or even more novel than the benchmarks crafted by humans. Meanwhile, human oversight remains effective through the use of controllable prompts, ensuring precise control over the content generated by these language models
- <span style="color:#ffc000">Intention and implication understanding</span> - Although models trained on the large-scale corpus are already intelligent enough to understand instructions, most are still incapable of emulating human dialogues or fully leveraging the information conveyed in language. Understanding the implied meanings is essential for effective communication and cooperation with other intelligent agents. When it comes to vague instructions or other implications, it poses a significant challenge for agents. For humans, grasping the implied meanings from a conversation comes naturally, whereas for agents, they should formalize implied meanings into a reward function that allows them to choose the option in line with the speaker’s preferences in unseen contextsOne of the main ways for reward modeling is inferring rewards based on feedback, which is primarily presented in the form of comparisons. 
### Knowledge

- Only the agents that acquire linguistic knowledge can comprehend sentences and engage in multi-turn conversations . Moreover, these agents can acquire multilingual knowledge by training on datasets that contain multiple languages, eliminating the need for extra translation models
- agents without commonsense knowledge may make incorrect decisions, such as not bringing an umbrella when it rains heavily.
- models designed to perform programming tasks need to possess programming knowledge, such as code format. Similarly, models intended for diagnostic purposes should possess medical knowledge like the names of specific diseases and prescription drugs.
#### Problems in knowledge acquiring
- the knowledge acquired by models during training could become outdated or even be incorrect from the start. A simple way to address this is retraining. However, it requires advanced data, extensive time, and computing resources. Even worse, it can lead to catastrophic forgetting
- Therefore, some researchers try editing LLMs to locate and modify specific knowledge stored within the models. This involved unloading incorrect knowledge while simultaneously acquiring new knowledge. Their experiments show that this method can partially edit factual knowledge, but its underlying mechanism still requires further research.
- Besides, LLMs may generate content that conflicts with the source or factual information, a phenomenon often referred to as <span style="color:#ffc000">hallucinations</span> . It is one of the critical reasons why LLMs can not be widely used in factually rigorous tasks. To tackle this issue, some researchers proposed a metric to measure the level of hallucinations and provide developers with an effective reference to evaluate the trustworthiness of LLM outputs. Moreover, some researchers enable LLMs to utilize external tools to avoid incorrect 13 knowledge
### Memory

- <span style="color:#ffc000">Raising the length limit of Transformers</span> - The first method tries to address or mitigate the inherent sequence length constraints. The Transformer architecture struggles with long sequences due to these intrinsic limits. As sequence length expands, computational demand grows exponentially due to the pairwise token calculations in the self-attention mechanism
- <span style="color:#ffc000">Summarizing memory</span> - The second strategy for amplifying memory efficiency hinges on the concept of memory summarization. This ensures agents effortlessly extract pivotal details from historical interactions. Various techniques have been proposed for summarizing memory. Using prompts, some methods succinctly integrate memories, while others emphasize reflective processes to create condensed memory representations. Hierarchical methods streamline dialogues into both daily snapshots and overarching summaries
- <span style="color:#ffc000">Methods for memory retrieval</span> - When an agent interacts with its environment or users, it is imperative to retrieve the most appropriate content from its memory. This ensures that the agent accesses relevant and accurate information to execute specific actions. Typically, agents retrieve memories in an automated manner. A significant approach in automated retrieval considers three metrics: Recency, Relevance, and Importance. The memory score is determined as a weighted combination of these metrics, with memories having the highest scores being prioritized in the model’s context
### Reasoning

- Reasoning, underpinned by evidence and logic, is fundamental to human intellectual endeavors, serving as the cornerstone for problem-solving, decision-making, and critical analysis. Deductive, inductive, and abductive are the primary forms of reasoning commonly recognized in intellectual endeavor
- Differing academic views exist regarding the reasoning capabilities of large language models. Some argue language models possess reasoning during pre-training or fine-tuning, while others believe it emerges after reaching a certain scale in size. Specifically, the representative Chain-of-Thought (CoT) method has been demonstrated to elicit the reasoning capacities of large language models by guiding LLMs to generate rationales before outputting the answer.
- <span style="color:#ffc000">Plan formulation</span> - During the process of plan formulation, agents generally decompose an overarching task into numerous sub-tasks, and various approaches have been proposed in this phase. Notably, some works advocate for LLM-based agents to decompose problems comprehensively in one go, formulating a complete plan at once and then executing it sequentially. In contrast, other studies like the CoT-series employ an adaptive strategy, where they plan and address sub-tasks one at a time, allowing for more fluidity in handling intricate tasks in their entirety. While LLM-based agents demonstrate a broad scope of general knowledge, they can occasionally face challenges when tasked with situations that require expertise knowledge. Enhancing these agents by integrating them with planners of specific domains has shown to yield better performance
- <span style="color:#ffc000">Plan reflection</span> - Upon formulating a plan, it’s imperative to reflect upon and evaluate its merits. LLM-based agents leverage internal feedback mechanisms, often drawing insights from pre-existing models, to hone and enhance their strategies and planning approaches. Furthermore, they could draw feedback from tangible or virtual surroundings, such as cues from task accomplishments or post-action observations, aiding them in revising and refining their plans
### Transferability and Generalization

- Intelligence shouldn’t be limited to a specific domain or task, but rather encompass a broad range of cognitive skills and abilities. The remarkable nature of the human brain is largely attributed to its high degree of plasticity and adaptability. It can continuously adjust its structure and function in response to external stimuli and internal needs, thereby adapting to different environments and tasks. 
- plenty of research indicates that pre-trained models on large-scale corpora can learn universal language representations
- Leveraging the power of pre-trained models, with only a small amount of data for fine-tuning, LLMs can demonstrate excellent performance in downstream tasks
- There is no need to train new models from scratch, which saves a lot of computation resources. However, through this task-specific fine-tuning, the models lack versatility and struggle to be generalized to other tasks. Instead of merely functioning as a static knowledge repository, LLM-based agents exhibit dynamic learning ability which enables them to adapt to novel tasks swiftly and robustly
- <span style="color:#ffc000">Unseen task generalization</span> - With the expansion of model size and corpus size, LLMs gradually exhibit remarkable emergent abilities in unfamiliar tasks without the need for task-specific fine-tuning. Specifically, LLMs can complete new tasks they do not encounter in the training stage by following the instructions based on their own understanding. One of the implementations is multi-task learning, for example, FLAN. Promisingly, such generalization capability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructions
- <span style="color:#ffc000">In-context learning</span> - Numerous studies indicate that LLMs can perform a variety of complex tasks through in-context learning (ICL), which refers to the models’ ability to learn from a few examples in the context. Few-shot in-context learning enhances the predictive performance of language models by concatenating the original input with several complete examples as prompts to enrich the context. The key idea of ICL is learning from analogy, which is similar to the learning process of humans. Unlike the supervised learning process, ICL doesn’t involve fine-tuning or parameter updates, which could greatly reduce the computation costs for adapting the models to new tasks.
- <span style="color:#ffc000">Continual learning</span> - A core challenge in continual learning is catastrophic forgetting: as a model learns new tasks, it tends to lose knowledge from previous tasks. Voyager attempts to solve progressively harder tasks proposed by the automatic curriculum devised by GPT-4. By synthesizing complex skills from simpler programs, the agent not only rapidly enhances its capabilities but also effectively counters catastrophic forgetting.
## Perception

![](../Images/Pasted%20image%2020230928214324.png)
- it’s crucial for LLM-based agents to receive information from various sources and modalities. This expanded perceptual space helps agents better understand their environment, make informed decisions, and excel in a broader range of tasks, making it an essential development direction. Agent handles this information to the Brain module for processing through the perception module. 
- <span style="color:#ffc000">Textual Input</span> - An LLM-based agent already has the fundamental ability to communicate with humans through textual input and output
	- In a user’s textual input, aside from the explicit content, there are also beliefs, desires, and intentions hidden behind it. Understanding implied meanings is crucial for the agent to grasp the potential and underlying intentions of human users, thereby enhancing its communication efficiency and quality with users.
	- understanding implied meanings within textual input remains challenging for the current LLM-based agent
	- as the agent is designed for use in complex real-world situations, it will inevitably encounter many entirely new tasks. Understanding text instructions for unknown tasks places higher demands on the agent’s text perception abilities.
- <span style="color:#ffc000">Visual Input</span> - Although LLMs exhibit outstanding performance in language comprehension and multi-turn conversations, they inherently lack visual perception and can only understand discrete textual content.
	- Visual input usually contains a wealth of information about the world, including properties of objects, spatial relationships, scene layouts, and more in the agent’s surroundings.
	- To help the agent understand the information contained within images, a straightforward approach is to generate corresponding text descriptions for image inputs, known as image captioning
	- Captions can be directly linked with standard text instructions and fed into the agent. This approach is highly interpretable and doesn’t require additional training for caption generation, which can save a significant number of computational resources.
	- However, caption generation is a low-bandwidth method, and it may lose a lot of potential information during the conversion process. Furthermore, the agent’s focus on images may introduce biases.
	- Inspired by the excellent performance of transformers in natural language processing, researchers have extended their use to the field of computer vision.
	- Researchers first divide an image into fixed-size patches and then treat these patches, after linear projection, as input tokens for Transformers. In the end, by calculating self-attention between tokens, they are able to integrate information across the entire image, resulting in a highly effective way to perceive visual content. Therefore, some works try to combine the image encoder and LLM directly to train the entire model in an end-to-end way. While the agent can achieve remarkable visual perception abilities, it comes at the cost of substantial computational resources.
	- Extensively pre-trained visual encoders and LLMs can greatly enhance the agent’s visual perception and language expression abilities. Freezing one or both of them during training is a widely adopted paradigm that achieves a balance between training resources and model performance. However, LLMs cannot directly understand the output of a visual encoder, so it’s necessary to convert the image encoding into embeddings that LLMs can comprehend. In other words, it involves aligning the visual encoder with the LLM. This usually requires adding an extra learnable interface layer between them.
	- Q-Former is a transformer that employs learnable query vectors, giving it the capability to extract language-informative visual representations. It can provide the most valuable information to the LLM, reducing the agent’s burden of learning visual-language alignment and thereby mitigating the issue of catastrophic forgetting.
	- Compared to image information, video information adds a temporal dimension. Therefore, the agent’s understanding of the relationships between different frames in time is crucial for perceiving video information. Some works like Flamingo ensure temporal order when understanding videos using a mask mechanism. The mask mechanism restricts the agent’s view to only access visual information from frames that occurred earlier in time when it perceives a specific frame in the video