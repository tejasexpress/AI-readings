- researchers have shown that models trained with these datasets are not actually learning very sophisticated language understanding and are instead largely drawing on simple pattern-matching heuristics (2017)
![Pasted image 20230927154918.png](../Images/Pasted%20image%2020230927154918.png)
## CoQA: Question Answering through Conversations

- Most current question answering systems are limited to answering questions independently (as the SQuAD examples shown above). Though this sort of question-answer exchange does sometimes happen between people, it is more common to seek information by engaging in conversations involving a series of interconnected questions and answers. CoQA is a 
  **Co**nversational **Q**uestion **A**nswering dataset that we developed to address this limitation with a goal of driving the development of conversational AI systems. ![Pasted image 20230927155054.png](../Images/Pasted%20image%2020230927155054.png)
  - An important feature is that we didn’t restrict the answers to be a contiguous span in the passage, as SQuAD does. We think that many questions are not able to be answered by a single span in the passage, which will limit the naturalness of the conversations. For example, for a question like _How many?_, the answer can be simply _three_ despite text in the passage not spelling this out directly. At the same time, we hope that our dataset supports a reliable automatic evaluation and obtains a high human agreement. To approach this, we asked the annotators to first highlight a text span (acting as a rationale to support the answer, see R1, R2 etc in the example) and then edit the text span into a natural answer. These rationales can be leveraged in training (but not in testing). 
  - Most existing QA datasets mainly focus on a single domain, which makes it hard to test the generalization ability of existing models. Another important feature of CoQA is that this dataset is collected from seven different domains — children’s stories, literature, middle and high school English exams, news, Wikipedia, Reddit and science. The last two are used for out-of-domain evaluation.
  - Compared to the question distribution of SQuAD 2.0, we find that our questions are much shorter than the SQuAD questions (5.5 vs 10.1 words on average), which reflects the conversational nature of our dataset. Our dataset also presents a richer variety of questions; while nearly half of SQuAD questions are dominated by _what_ questions, the distribution of CoQA is spread across multiple question types. Several sectors indicated by prefixes _did_, _was_, _is_, _does_ are frequent in CoQA but are completely absent in SQuAD.![Pasted image 20230927155355.png](../Images/Pasted%20image%2020230927155355.png)
  - The state-of-the-art ensemble system “BERT+MMFT+ADA” from Microsoft Research Asia achieved 87.5% in-domain F1 accuracy and 85.3% out-of-domain F1 accuray.
## HotpotQA: Machine Reading over Multiple Documents

- The Web contains the answers to many of these questions, but not always in a readily available form, or even in one place. For example, if we take Wikipedia as the source of knowledge to answer our first question (about where Yahoo! was founded), we will initially be baffled that none of the pages of Yahoo! or those of its co-founders Jerry Yang and David Filo mention this information.
- To answer this question, one would need to laboriously browse multiple Wikipedia articles, until they come across the following article titled History of Yahoo!
- As one can see, we can answer the question in the following steps of reasoning:

	- We note that the first sentence of this article states that _Yahoo!_ was founded at [Stanford University](https://en.wikipedia.org/wiki/Stanford_University).
	- Then, we can look up Stanford University in Wikipedia (in this case we simply clicked on the link), to find out where it’s located in
	- The Stanford University page tells us that it is located in California.
	- Finally, we can combine these two facts to arrive at the answer to the original question: _Yahoo!_ was founded in the State of _California_.
- Note that to answer this question, two skills were essential: (1) _a bit of detective work_ to find out about what documents, or supporting facts, to use that could lead to an answer to our question, and (2) the ability to _reason with multiple supporting facts_ to arrive at the final answer
- These are important capabilities for machine reading systems to acquire in order for them to effectively assist us in digesting the ever-growing ocean of information and knowledge in the form of text. Unfortunately, because existing datasets have thus focused on finding answers within single documents, falling short at tackling this challenge, we undertook the effort of making that possible by compiling the HotpotQA dataset.![Pasted image 20230927155621.png](../Images/Pasted%20image%2020230927155621.png)![Pasted image 20230927155633.png](../Images/Pasted%20image%2020230927155633.png)
- The questions require many challenging types of reasoning to answer. For example, in the _Yahoo!_ example, one would need to first infer the relation between Yahoo! and the “missing link” essential to answering the question, _Stanford University_, and then leverage the fact that _Stanford University_ is located in _California_ to arrive at the final answer.
- Obviously, these bridge entity questions probably don’t cover all of the interesting questions one could try to answer by reasoning over multiple facts collected on Wikipedia. In HotpotQA, we include a new type of questions – comparison questions – to represent a more diverse set of reasoning skills and language understanding capabilities. _does Stanford have more computer science researchers or Carnegie Mellon University?_
- To successfully answer these questions, a QA system needs to be able to not only find the relevant supporting facts (in this case, how many computer science researchers Stanford and CMU have, respectively), but also to compare them in a meaningful way to yield the final answer. The latter could prove quite challenging for current QA systems, as our analysis of the dataset show, because it could involve numerical comparison, time comparison, counting, and even simple arithmetic ( which most natural language processing models are not good at)
- Another important and desirable trait of good question answering systems is _explainability_. In fact, a QA system that simply spits out an answer with no explanation or demonstrations to help verify its answers is almost useless, because the user wouldn’t be able to trust its answers even if they appear to be correct most of the time. Unfortunately, this has been a problem with many state-of-the-art question answering systems. To this end, when collecting the data for HotpotQA we also asked our annotators to specify the supporting sentences they used to arrive at the final answer, and released these as part of the dataset